import dask.dataframe as dd
from dask.dataframe import from_pandas
# Очистка и подготовка данных
# После импорта данных в Dask, часто требуется провести их очистку и предобработку, чтобы сделать их
# пригодными для анализа.
# 1. Удаление дубликатов:
# Дубликаты в данных могут исказить результаты анализа. Dask позволяет легко найти и удалить дубликаты. Например:
df3=dd.read_csv('file_exz.csv').set_index('Index')
print('Вывод пяти первых строк загруженного датафрейма')
print('*'*60)
print(df3.head())
print('Количество строк')
print(len(df3))
# Удаление дубликатов из Dask DataFrame
# df.drop_duplicates(subset=['col0', 'col1'], keep='last') удаление дубликатов по столбцам
# Чтобы удалить дубликаты и сохранить последние вхождения, используйте keep.
df_db=df3.drop_duplicates(subset=['First Name'])
print('*'*60)
print('Удаление дубликатов в столбце: First Name ')
print(df_db.tail())
print(len(df_db))

df3 = df3.drop_duplicates()
print('*'*60)
print('Удаление дубликатов строк')
print(df3.head())
print(df3.tail())
print(df3.dtypes)
# удаление дубликата столбца
print('Удаление дубликатов столбцов')
df4=df3.compute()
df4=df4.T.drop_duplicates().T
df4 = from_pandas(df4)
print('*'*60)
print(df4.dtypes)
print(df4.head())
print(len(df4))
print('*'*60)
print('Метод describe(include="all")')
print(df3.describe(include='all'))
print('Метод info()')
print(df3.info())

# 2. Обработка отсутствующих значений:
# Отсутствующие значения (NaN или None) могут быть проблемой при анализе данных. Dask предоставляет
# методы для обработки отсутствующих значений, такие как fillna и dropna:
# Заполнение отсутствующих значений в Dask DataFrame
df3 = df3.fillna(0)  # Заменить NaN на 0
print('*'*60)
print('Замена NaN на 0')
print(df3.head())
print(len(df3))

# Удаление строк с отсутствующими значениями
# df3 = df3.dropna()

# Преобразование столбца 'age' в числовой тип данных
df3['age'] = df3['age'].astype(int)
print('*'*60)
print('Преобразование столбца - age')
print(df3)

# Создание нового признака 'income_per_age'
df3['income_per_age'] = df3['amount_spent'] / df3['age']

# df3['age_level'] = df3['age'].apply(lambda x: 'yang' if x < 30 else 'midlle' if x < 60 else 'old')
print('*'*60)
print('Новый столбец: income_per_age')
print(df3)

# 4. Фильтрация данных:
# Фильтрация данных позволяет выбрать только необходимую часть данных для дальнейшего анализа.
# Dask предоставляет метод loc для выполнения фильтрации:
# Фильтрация данных: выбор только клиентов старше 30 лет
df_filtered = df3.loc[df3['age'] > 30]
print('*'*60)
print('ФИЛЬТРАЦИЯ по age > 30')
print(df_filtered.head())
print(len(df_filtered))
# Работа с большими объемами данных с помощью Dask
# Одним из ключевых преимуществ Dask является его способность работать с большими объемами данных,
# которые не помещаются в оперативной памяти одного компьютера. Это достигается за счет разделения
# данных на блоки и выполнения вычислений параллельно:
# Рассчет среднего значения столбца 'amount_spent'

mean_value = df3['amount_spent'].mean()

# Вычисление результата
result = mean_value.compute()
print('*'*60)
print(f'Столбец amount_spent: Среднее значение - {result}.')

# В этом примере мы сначала читаем большой файл CSV с использованием Dask. Затем мы вычисляем среднее
# значение столбца value. Обратите внимание, что мы не вызываем .compute() сразу после вычисления
# среднего значения. Вместо этого Dask создает граф задач, который представляет собой
# последовательность вычислительных шагов. Когда мы вызываем .compute(), Dask выполняет эти
# вычисления параллельно или распределенно, в зависимости от конфигурации.
# Это позволяет Dask эффективно обрабатывать данные, которые могут быть слишком большими для обычных
# инструментов анализа данных, и делает его мощным инструментом для работы с крупными объемами
# информации.

# Группировка и агрегация:
# Dask DataFrames позволяют выполнять группировку данных по определенным столбцам и
# вычислять агрегатные функции, такие как сумма, среднее значение и медиана:
# Группировка данных по столбцу 'Country' и вычисление суммы
grouped = df3.groupby('Country')['amount_spent'].sum()
print('*'*60)
result = grouped.compute()
print(f'Группировка данных по столбцу "Country" и вычисление суммы столбца "amount_spent": {result}.')

# Группировка данных и вычисление нескольких агрегатных функций
aggregated = df3.groupby('Country')['amount_spent'].agg(['sum', 'mean', 'median'])
print('*'*60)
print('Группировка данных по столбцу "Country" и вычисление суммы, среднего и мидианы столбца "amount_spent":')
print(aggregated.compute())

# Сортировка данных:
# Dask DataFrames позволяют выполнять сортировку данных по одному или нескольким столбцам:

# Сортировка данных по столбцу 'age' по убыванию
sorted_df = df3.sort_values(by='age', ascending=False)
sorted_df.compute()
print('*'*60)
print('Сортировка данных по столбцу "age" по убыванию')
print(sorted_df.head(7))
#
# Параллельные и распределенные вычисления являются ключевыми аспектами анализа данных при
# работе с большими объемами информации.
# Dask обеспечивает встроенную поддержку параллельных вычислений, что позволяет использовать
# все доступные ядра процессора для ускорения вычислений. Это особенно полезно при выполнении
# операций на данных, которые можно разбить на части и обработать параллельно.
# Dask самостоятельно управляет параллельными вычислениями и оптимизирует их выполнение.
# Dask также поддерживает параллельные вычисления с Dask DataFrames. При выполнении операций
# на Dask DataFrames, Dask разбивает данные на блоки по столбцам и строкам и выполняет операции
# параллельно:

# Вычисление суммы столбца 'salary' с использованием параллельных вычислений
total_salary = df3['amount_spent'].sum()
#
# Автоматически выполняет параллельные вычисления
result = total_salary.compute()
print('*'*60)
print(f'Столбец amount_spent: сумма элементов - {result}.')
# Важно отметить, что Dask обеспечивает прозрачное выполнение параллельных вычислений
# без необходимости явно управлять потоками или процессами.

# Масштабирование Dask для работы с большими вычислительными кластерами*
#
# Одним из сильных преимуществ Dask является его способность масштабирования для работы
# с большими вычислительными кластерами.
# 1. Создание вычислительного кластера:
#
# Для начала работы с распределенными вычислениями с Dask, вы можете создать вычислительный
# кластер. Это может быть локальный кластер, который использует все доступные ядра на вашей
# машине, или удаленный кластер на нескольких машинах.
# from dask.distributed import LocalCluster, Client

# Создание локального кластера
# cluster = LocalCluster()
# подключение клиента к кластеру
# client = Client(cluster)
#
# Пример создания удаленного кластера:
# from dask.distributed import Client
#
# Подключение клиента к удаленному кластеру
# client = Client('scheduler_address:8786')
#
# Важно иметь в виду, что для удаленного кластера требуется настройка доступной
# инфраструктуры, такой как Kubernetes или Apache Mesos.
#